{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import netcomp as nc\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from scipy import spatial\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import string\n",
    "import pysolr\n",
    "import urllib\n",
    "import requests\n",
    "url_host = 'http://localhost:8983/solr/Breast_cancer_guidelines'\n",
    "\n",
    "model= gensim.models.KeyedVectors.load_word2vec_format(\"cleaned_gensim_loadable\",binary=True)\n",
    "model2 = gensim.models.KeyedVectors.load('Model_conceptualized_300v_9w_25i_Nopid_mincount15_intersect')\n",
    "model3 = gensim.models.KeyedVectors.load('Model_300v_9w_15i_Nopid_mincount15_intersect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "concept_to_mesh={}\n",
    "with open(\"Mesh_to_concepts_Extended.txt\",'r') as M:\n",
    "    for line in M:\n",
    "        mesh=line.split(\":#:#:#:##:##\")[0].split(\":#:#:#:\")[0]\n",
    "        for term in line.replace(\"\\n\",\"\").split(\":#:#:#:##:##\")[1].split(\"##:##\"):\n",
    "            concept_to_mesh.update({re.sub(r'\\s+', '_',re.sub(r'[^\\w\\s]', ' ', term).lower()):re.sub(r'\\s+', '_',re.sub(r'[^\\w\\s]', ' ', mesh).lower())})\n",
    "\n",
    "con_init=set([])\n",
    "for concept in concept_to_mesh:\n",
    "    con_init.add(concept.split(\"_\")[0])\n",
    "def ngram_replacer_bioasq(line):\n",
    "    query_list=list(filter(None,re.sub(r'\\s+', ' ',re.sub(r'[^\\w\\s]', ' ', line).lower()).split(\" \")))\n",
    "    i=0\n",
    "    query_list_len=len(query_list)\n",
    "    query_ngram_list=[]\n",
    "    while i<query_list_len:\n",
    "        if i<query_list_len-4:\n",
    "            if \"-\".join(query_list[i:i+5]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3],query_list[i+4]]))\n",
    "                i+=5\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+4]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-3: \n",
    "            if \"-\".join(query_list[i:i+4])in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-2: \n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2])in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-1: \n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "\n",
    "        query_ngram_list.append(query_list[i])\n",
    "        i+=1\n",
    "    return (' '.join(query_ngram_list))\n",
    "def ngram_replacer_list_bioasq(line):\n",
    "    query_list=[re.sub(r'\\s+', ' ',w).lower() for w in line]\n",
    "    i=0\n",
    "    query_list_len=len(query_list)\n",
    "    query_ngram_list=[]\n",
    "    while i<query_list_len:\n",
    "        if i<query_list_len-4:\n",
    "            if \"-\".join(query_list[i:i+5]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3],query_list[i+4]]))\n",
    "                i+=5\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+4]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-3: \n",
    "            if \"-\".join(query_list[i:i+4])in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-2: \n",
    "            if \"-\".join(query_list[i:i+3]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"-\".join(query_list[i:i+2])in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-1: \n",
    "            if \"-\".join(query_list[i:i+2]) in model.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "\n",
    "        query_ngram_list.append(query_list[i])\n",
    "        i+=1\n",
    "    return (' '.join(query_ngram_list))\n",
    "def ngram_replacer(line):\n",
    "    query_list=list(filter(None,re.sub(r'\\s+', ' ',re.sub(r'[^\\w\\s]', ' ', line).lower()).split(\" \")))\n",
    "    i=0\n",
    "    query_list_len=len(query_list)\n",
    "    query_ngram_list=[]\n",
    "    while i<query_list_len:\n",
    "        if i<query_list_len-4:\n",
    "            if \"_\".join(query_list[i:i+5]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3],query_list[i+4]]))\n",
    "                i+=5\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+4]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+3]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+2]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-3: \n",
    "            if \"_\".join(query_list[i:i+4])in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2],query_list[i+3]]))\n",
    "                i+=4\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+3]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+2]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"-\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-2: \n",
    "            if \"_\".join(query_list[i:i+3]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1],query_list[i+2]]))\n",
    "                i+=3\n",
    "                continue\n",
    "            if \"_\".join(query_list[i:i+2])in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "        elif i<query_list_len-1: \n",
    "            if \"_\".join(query_list[i:i+2]) in model2.wv.vocab:\n",
    "                query_ngram_list.append(\"_\".join([query_list[i],query_list[i+1]]))\n",
    "                i+=2\n",
    "                continue\n",
    "\n",
    "        query_ngram_list.append(query_list[i])\n",
    "        i+=1\n",
    "    return (' '.join(query_ngram_list))\n",
    "\n",
    "def ngram_replacer_list(line):\n",
    "    query_list=list(filter(None,re.sub(r'\\s+', ' ',re.sub(r'[^\\w\\s]', ' ', line).lower()).split(\" \")))\n",
    "    query_ngram_list=[]\n",
    "    if len(query_list)==1:\n",
    "        if query_list[0] in concept_to_mesh:\n",
    "            return concept_to_mesh[query_list[0]]\n",
    "        return (query_list[0])\n",
    "    term=\"\"\n",
    "    i=0\n",
    "    while i<len(query_list):\n",
    "        term=query_list[i]\n",
    "        temp_i=i\n",
    "        if query_list[i] in con_init:\n",
    "            for j in range(i,len(query_list)+1):\n",
    "                if '_'.join(query_list[i:j]) in concept_to_mesh:\n",
    "                    term='_'.join(query_list[i:j])\n",
    "                    temp_i=j\n",
    "        if temp_i==i:\n",
    "            query_ngram_list.append(term)\n",
    "        else:\n",
    "            query_ngram_list.append(concept_to_mesh[term])\n",
    "            i=temp_i-1\n",
    "        i+=1      \n",
    "    return (' '.join(query_ngram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACP.txt::ACP.txt\n",
      "ACP.txt::ACOG.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACP.txt::AAFP.txt\n",
      "ACOG.txt::ACP.txt\n",
      "ACOG.txt::ACOG.txt\n",
      "ACOG.txt::AAFP.txt\n",
      "AAFP.txt::ACP.txt\n",
      "AAFP.txt::ACOG.txt\n",
      "AAFP.txt::AAFP.txt\n",
      "[[0, 2.098817721575624, 2.3459521105228394], [2.0988177215756227, 0, 2.30872160107019], [2.3459521105228367, 2.3087216010701948, 0]]\n"
     ]
    }
   ],
   "source": [
    "distances=[]\n",
    "distances_m=[]\n",
    "distances_m2=[]\n",
    "sims=[]\n",
    "sims_m=[]\n",
    "sims_m2=[]\n",
    "Done_list=[]\n",
    "for root, dirs, files in os.walk('Full_Text/'):\n",
    "    for file in files:\n",
    "            with open(root+\"/\"+file,\"r\",encoding=\"ISO-8859-1\")as f1:\n",
    "                doc_id=file.replace(\".txt\",\"\")\n",
    "                distance=[]\n",
    "                distance_m=[]\n",
    "                distance_m2=[]\n",
    "                nsim=[]\n",
    "                nsim_m=[]\n",
    "                nsim_m2=[]\n",
    "                f1_t=f1.read().replace(\"\\n\",\" \")\n",
    "                f1_text= ngram_replacer_bioasq(f1_t)\n",
    "                f1_text_m= ngram_replacer_list(f1_t)\n",
    "                f1_tokens=[w.lower() for w in f1_text.split() if w.lower() in model.vocab and w not in stop_words]\n",
    "                f1_tokens_m=[w.lower() for w in f1_text_m.split() if w.lower() in model2.wv.vocab and w not in stop_words]\n",
    "                f1_tokens_m2=[w.lower() for w in f1_t.split() if w.lower() in model3.wv.vocab and w not in stop_words]\n",
    "                for root2, dirs2, files2 in os.walk('Full_Text/'):\n",
    "                    for file2 in files2:\n",
    "                            doc_id2=file2.replace(\".txt\",\"\")\n",
    "                            print(file+\"::\"+file2)\n",
    "                            with open(root+\"/\"+file2,\"r\",encoding=\"ISO-8859-1\")as f2:\n",
    "                                f2_t=f2.read().replace(\"\\n\",\" \")\n",
    "                                f2_text= ngram_replacer_bioasq(f2_t)\n",
    "                                f2_text_m= ngram_replacer_list(f2_t)\n",
    "\n",
    "                                if not file==file2:\n",
    "\n",
    "                                    f2_tokens=[w.lower() for w in f2_text.split() if w.lower() in model.vocab and w not in stop_words]\n",
    "\n",
    "\n",
    "                                    f2_tokens_m=[w.lower() for w in f2_text_m.split() if w.lower() in model2.wv.vocab and w not in stop_words]\n",
    "\n",
    "                                    f2_tokens_m2=[w.lower() for w in f2_t.split() if w.lower() in model3.wv.vocab and w not in stop_words]\n",
    "\n",
    "                                    sim=model.wmdistance(f1_tokens,f2_tokens)\n",
    "                                    sim_m=model2.wv.wmdistance(f1_tokens_m,f2_tokens_m)\n",
    "                                    sim_m2=model3.wv.wmdistance(f1_tokens_m2,f2_tokens_m2)\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim=0\n",
    "                                    distance.append(0)\n",
    "                                else:\n",
    "                                    distance.append(sim)\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim_m=0\n",
    "                                    distance_m.append(0)\n",
    "                                else:\n",
    "                                    distance_m.append(sim_m)\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim_m2=0\n",
    "                                    distance_m2.append(0)\n",
    "                                else:\n",
    "                                    distance_m2.append(sim_m2)\n",
    "\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim=1\n",
    "                                    nsim.append(1)\n",
    "                                else:\n",
    "                                    sim=model.wv.n_similarity(f1_tokens,f2_tokens)\n",
    "                                    nsim.append(sim)\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim_m=1\n",
    "                                    nsim_m.append(1)\n",
    "                                else:\n",
    "                                    sim_m=model2.wv.n_similarity(f1_tokens_m,f2_tokens_m)\n",
    "                                    nsim_m.append(sim_m)\n",
    "\n",
    "                                if file==file2:\n",
    "                                    sim_m2=1\n",
    "                                    nsim_m2.append(1)\n",
    "                                else:\n",
    "                                    sim_m2=model3.wv.n_similarity(f1_tokens_m2,f2_tokens_m2)\n",
    "                                    nsim_m2.append(sim_m2)\n",
    "    #                         print(\"n_sim:\"+doc_id+\"--\"+doc_id2+\":\"+str(sim_m))\n",
    "                distances.append(distance)\n",
    "                distances_m.append(distance_m)\n",
    "                distances_m2.append(distance_m2)\n",
    "                sims.append(nsim)\n",
    "                sims_m.append(nsim_m)\n",
    "                sims_m2.append(nsim_m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distortion:\n",
      "0.0\n",
      "Average distortion on permutations:\n",
      "0.24126984126984263\n",
      "Standard deviation of distortions:\n",
      "0.0046736215858919836\n"
     ]
    }
   ],
   "source": [
    "###Distances provided by experts\n",
    "ad=[[0, 2, 3, 3, 2, 2, 1],\n",
    "   [2, 0, 4, 1, 2, 2, 1],\n",
    "   [3, 4, 0, 3, 2, 3, 3],\n",
    "   [3, 1, 3, 0, 1, 2, 2],\n",
    "   [2, 2, 2, 1, 0, 1, 1],\n",
    "   [2, 2, 3, 2, 1, 0, 1],\n",
    "   [1, 1, 3, 2, 1, 1, 0]]\n",
    "ad_dist = np.array(ad).astype(float)\n",
    "\n",
    "###select your desired model and ditance type\n",
    "\n",
    "d =  np.array(distances).astype(float)\n",
    "# d =  np.array(distances_m).astype(float)\n",
    "# d =  np.array(distances_m2).astype(float)\n",
    "# d =  np.array(sims).astype(float)\n",
    "# d =  np.array(sims_m).astype(float)\n",
    "# d =  np.array(sims_m2).astype(float)\n",
    "\n",
    "\n",
    "dt = [('len', float)]\n",
    "# print(d)\n",
    "# print(ad_dist)\n",
    "\n",
    "### for nsim:\n",
    "\n",
    "#d=1-d\n",
    "### OR\n",
    "# d=1/d-1\n",
    "\n",
    "\n",
    "d_c=d\n",
    "print(\"Distortion:\")\n",
    "print(abs(d/d.sum()-ad_dist/ad_dist.sum()).sum())\n",
    "for root, dirs, files in os.walk('Full_Text/'):\n",
    "    \n",
    "    g_list=list(files)\n",
    "#             g_list\n",
    "count=0\n",
    "sum=0\n",
    "sum_std=0\n",
    "sum_std2=0\n",
    "g=g_list\n",
    "\n",
    "###Computing average dostortion for different permutations\n",
    "###Modify base on the number of the documents\n",
    "\n",
    "for g in g_list:\n",
    "    g_list2=g_list.copy()\n",
    "    g_list2.remove(g)\n",
    "    for g2 in g_list2:\n",
    "        g_list3=g_list2.copy()\n",
    "        g_list3.remove(g2)\n",
    "        for g3 in g_list3:\n",
    "            g_list4=g_list3.copy()\n",
    "            g_list4.remove(g3)\n",
    "            for g4 in g_list4:\n",
    "                g_list5=g_list4.copy()\n",
    "                g_list5.remove(g4)\n",
    "                for g5 in g_list5:\n",
    "                    g_list6=g_list5.copy()\n",
    "                    g_list6.remove(g5)\n",
    "                    for g6 in g_list6:\n",
    "                        g_list7=g_list6.copy()\n",
    "                        g_list7.remove(g6)\n",
    "                        g7=g_list7.pop()\n",
    "                        count=count+1\n",
    "                        g_l=[g,g2,g3,g4,g5,g6,g7]\n",
    "                        d=d_c\n",
    "                        d = d.astype(float)\n",
    "                        for gg in g_list:\n",
    "                            i=g_list.index(gg)\n",
    "                            j=g_l.index(gg)\n",
    "                            d[[i,j]]=d[[j,i]]\n",
    "                            d[:,[i,j]]=d[:,[j,i]]\n",
    "                            g_l[i],g_l[j]=g_l[j],g_l[i]\n",
    "\n",
    "\n",
    "                        dt = [('len', float)]\n",
    "                        d= d/d.sum()\n",
    "                        ad_dist=ad_dist/ad_dist.sum()\n",
    "                        sum_std2=sum_std2+np.std(abs(d-ad_dist))\n",
    "                        sum=sum+abs(d-ad_dist).sum()\n",
    "print(\"Average distortion on permutations:\")\n",
    "print(sum/count)\n",
    "print(\"Standard deviation of distortions:\")\n",
    "print(sum_std2/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "ad_dist=[[0, 2, 3, 3, 2, 2, 1],\n",
    "       [2, 0, 4, 1, 2, 2, 1],\n",
    "       [3, 4, 0, 3, 2, 3, 3],\n",
    "       [3, 1, 3, 0, 1, 2, 2],\n",
    "       [2, 2, 2, 1, 0, 1, 1],\n",
    "       [2, 2, 3, 2, 1, 0, 1],\n",
    "       [1, 1, 3, 2, 1, 1, 0]]\n",
    "ad_dist=np.array(ad_dist)\n",
    "\n",
    "d=ad_dist\n",
    "d = d.astype(float)\n",
    "dt = [('len', float)]\n",
    "A=d\n",
    "\n",
    "A = A.round(4)\n",
    "A = A.view(dt)\n",
    "G = nx.from_numpy_matrix(A)\n",
    "\n",
    "G = nx.relabel_nodes(G, {0: 'AAFP', 1: 'ACOG', 2: 'ACP', 3: 'ACR', 4: 'ACS', 5: 'IARC', 6: 'USPSTF'})\n",
    "min_ed=100000\n",
    "max_ed=0\n",
    "for (u,v,d) in G.edges(data='len'):\n",
    "    if d<min_ed:\n",
    "        min_ed=d\n",
    "    if d>max_ed:\n",
    "        max_ed=d\n",
    "\n",
    "pos = nx.shell_layout(G,scale=1)\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(G,'len')\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "plot_margin = 1.25\n",
    "\n",
    "x0, x1, y0, y1 = plt.axis()\n",
    "plt.axis((x0 - plot_margin,\n",
    "          x1 + plot_margin,\n",
    "          y0 - plot_margin,\n",
    "          y1 + plot_margin))\n",
    "\n",
    "G = nx.drawing.nx_agraph.to_agraph(G)\n",
    "\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels = edge_labels)\n",
    "nx.draw_networkx_labels(G,pos,font_size=10)\n",
    "nx.draw_networkx_nodes(G,pos, node_color='yellow', node_size=1200)\n",
    "nx.draw(G, pos,node_color=['yellow','yellow','yellow','yellow','yellow','yellow','yellow'])\n",
    "  \n",
    "\n",
    "# print(A)\n",
    "# print(G) \n",
    "\n",
    "G.node_attr.update(color=\"yellow\", style=\"filled\",font_size = \"2\")\n",
    "G.edge_attr.update(color=\"black\", width=\"2.0\")\n",
    "# nx.draw_networkx_labels(G,pos)\n",
    "G.draw('Conceptual_distance.png', format='png', prog='neato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
